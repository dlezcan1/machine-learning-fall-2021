Keep a table of rewards for $R(s_0, s_1,a)$ (2 values to update here). As you iterate through value iteration. Then, after action, $a$, is executed in value iteration, sample the reward and append the sampled reward if $a$ takes you from an initial state $s_0 \rightarrow s_1$ to the action $a$ part of the table. Then, when evaluating the $Q$ function of value iteration, use the mean of the list of rewards $R(s_0, s_1, a)$ when as the $Q$ function. 
Here, we have in the limit as $t \rightarrow \infty$, $\mean \{R_t(s_0, s_1, a)\} \rightarrow \expect{R_\pi(s)(s,s^{(1)})}$ (the actual expected reward.