\tftrue

As per slide 8 of the Ensemble Methods lecture, it is stated that "[one] can show Super Learner does \emph{as well as the best possibly weighted combination in} [the hypothesis class] as $N \rightarrow \infty$. Should we contain the full distribution, obtained by an infinite sample size, take any single learner to be the best performing learner, a Super Learner could learn the weighting that that only weights the best performing learner with $\alpha = 1$ and the rest $0$. Therefore, any other improvements to learning by varying $\alpha$ parameters are lower-bounded by the performance of the best performing single-learner.