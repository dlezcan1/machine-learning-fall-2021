\tftrue

For each class $Y_l$ (say $l = 1, \dots, k$), define a perceptron with parameters $\beta_l$ such that 
$f_l(x) = \sign(\beta_l^T x)$ (including bias term in dot product) be the decision rule for $x$ to be classified as $Y_l$ or not $Y_l$. So here, we have $k$ perceptron classifiers, $f_l$, that determines whether each is either in a specific class or not. Since all of the classes are linearly separable, the class $Y_l$ is linearly separable with not $Y_l$ classes. So we train each perceptron to discriminate what is $Y_l$ and what is not $Y_l$ for $l = 1, \dots, k$, since these classes are linearly separable with what is $Y_l$ and what is not $Y_l$. 